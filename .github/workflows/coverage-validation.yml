name: Test Coverage Validation

on:
  pull_request:
    branches: [main, master]
    types: [opened, synchronize, reopened, ready_for_review]

jobs:
  coverage-validation:
    runs-on: ubuntu-latest
    if: github.event.pull_request.draft == false
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install requests dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-cov

    - name: Install cdlreq for coverage analysis
      run: |
        # Clone and install cdlreq (assuming it's available on GitHub)
        # Adjust this step based on where cdlreq is hosted
        git clone https://github.com/SeanHgh/cdlreq.git /tmp/cdlreq || echo "cdlreq clone failed, using alternative method"
        
        # Try to install from parent directory if available
        if [ -d "../cdlreq" ]; then
          echo "Using local cdlreq from parent directory"
          pip install -e ../cdlreq
        elif [ -d "/tmp/cdlreq" ]; then
          echo "Using cloned cdlreq"
          pip install -e /tmp/cdlreq
        else
          echo "Installing cdlreq from PyPI or falling back"
          pip install pyyaml jsonschema click openpyxl  # cdlreq dependencies
          # For now, we'll copy the cdlreq code inline if needed
        fi

    - name: Run pytest and capture output
      run: |
        echo "🧪 Running pytest and capturing output for coverage analysis..."
        python -m pytest tests/ -v --tb=short > test_output.txt 2>&1 || true
        echo "📋 Test output captured to test_output.txt"
        echo ""
        echo "Test output preview (first 30 lines):"
        head -30 test_output.txt
        echo ""
        echo "Test output preview (last 10 lines):"
        tail -10 test_output.txt

    - name: Validate test coverage using cdlreq
      id: coverage_check
      run: |
        echo "🔍 Running cdlreq coverage analysis..."
        
        # Try different ways to run cdlreq coverage
        COVERAGE_EXIT_CODE=0
        COVERAGE_OUTPUT=""
        
        # Method 1: Try installed cdlreq
        if command -v cdlreq &> /dev/null; then
          echo "Using installed cdlreq command"
          COVERAGE_OUTPUT=$(cdlreq coverage test_output.txt --directory . 2>&1) || COVERAGE_EXIT_CODE=$?
        # Method 2: Try Python module
        elif python -c "import cdlreq" 2>/dev/null; then
          echo "Using cdlreq Python module"
          COVERAGE_OUTPUT=$(python -c "from cdlreq.cli.commands import cli; cli()" coverage test_output.txt --directory . 2>&1) || COVERAGE_EXIT_CODE=$?
        # Method 3: Use inline implementation as fallback
        else
          echo "Using fallback coverage analysis"
          
          # Simple Python script to analyze coverage
          python3 << 'EOF' > coverage_analysis.py
import yaml
import os
from pathlib import Path

def analyze_coverage():
    print("📊 Analyzing test coverage...")
    
    # Find all specification files
    specs_dir = Path("requirements/specifications")
    if not specs_dir.exists():
        print("❌ No specifications directory found")
        return 1
    
    # Read test output
    try:
        with open("test_output.txt", "r") as f:
            test_output = f.read()
    except FileNotFoundError:
        print("❌ Test output file not found")
        return 1
    
    # Analyze specifications
    invalid_files = {}
    executed = set()
    not_executed = set()
    
    for spec_file in specs_dir.glob("*.yaml"):
        try:
            with open(spec_file, "r") as f:
                spec = yaml.safe_load(f)
            
            if "unit_test" in spec and spec["unit_test"]:
                test_path = spec["unit_test"]
                spec_id = spec.get("id", "unknown")
                
                # Check if test file exists
                if Path(test_path).exists():
                    # Check if test was executed
                    if test_path in test_output:
                        executed.add((test_path, spec_id))
                    else:
                        not_executed.add((test_path, spec_id))
                else:
                    if test_path not in invalid_files:
                        invalid_files[test_path] = []
                    invalid_files[test_path].append(spec_id)
        
        except Exception as e:
            print(f"⚠️  Error processing {spec_file}: {e}")
    
    # Display results
    if invalid_files:
        print("⚠️  Invalid test files (do not exist):")
        for test_path, spec_ids in invalid_files.items():
            spec_list = ", ".join(spec_ids)
            print(f"  {test_path} → used in: {spec_list}")
        print()
    
    if executed:
        print("✅ Executed tests:")
        for test_path, spec_id in sorted(executed):
            print(f"  {test_path} → used in: {spec_id}")
        print()
    
    if not_executed:
        print("❌ Not executed:")
        for test_path, spec_id in sorted(not_executed):
            print(f"  {test_path} → used in: {spec_id}")
        print()
    
    # Summary
    valid_tests = len(executed) + len(not_executed)
    print(f"{len(executed)}/{valid_tests} valid tests executed")
    if invalid_files:
        print(f"{len(invalid_files)} invalid test file(s) in specifications")
    
    # Return exit code
    if not_executed or invalid_files:
        return 1
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(analyze_coverage())
EOF
          
          COVERAGE_OUTPUT=$(python3 coverage_analysis.py 2>&1) || COVERAGE_EXIT_CODE=$?
        fi
        
        echo "Coverage analysis output:"
        echo "$COVERAGE_OUTPUT"
        echo ""
        
        # Check results
        if echo "$COVERAGE_OUTPUT" | grep -q "❌ Not executed:" || echo "$COVERAGE_OUTPUT" | grep -q "⚠️  Invalid test files"; then
          echo "❌ COVERAGE VALIDATION FAILED"
          echo "Some specification unit test files are not executed or invalid"
          echo "coverage_passed=false" >> $GITHUB_OUTPUT
          echo "coverage_output<<EOF" >> $GITHUB_OUTPUT
          echo "$COVERAGE_OUTPUT" >> $GITHUB_OUTPUT  
          echo "EOF" >> $GITHUB_OUTPUT
          exit 1
        else
          echo "✅ COVERAGE VALIDATION PASSED"
          echo "All specification unit test files are properly covered"
          echo "coverage_passed=true" >> $GITHUB_OUTPUT
          echo "coverage_output<<EOF" >> $GITHUB_OUTPUT
          echo "$COVERAGE_OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          exit 0
        fi

    - name: Comment on PR with coverage results
      if: failure() || success()
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const fs = require('fs');
          const coveragePassed = '${{ steps.coverage_check.outputs.coverage_passed }}' === 'true';
          const coverageOutput = `${{ steps.coverage_check.outputs.coverage_output }}`;
          
          const icon = coveragePassed ? '✅' : '❌';
          const status = coveragePassed ? 'PASSED' : 'FAILED';
          const color = coveragePassed ? '🟢' : '🔴';
          
          let body = `## ${icon} Test Coverage Validation ${status}\n\n`;
          
          if (coveragePassed) {
            body += `${color} **All specification unit tests are properly covered!**\n\n`;
            body += `All unit test files referenced in your specifications were executed during pytest testing.\n\n`;
          } else {
            body += `${color} **Test coverage validation failed!**\n\n`;
            body += `❌ **This pull request cannot be merged** until all issues are resolved.\n\n`;
            body += `Some unit test files referenced in specifications were either:\n`;
            body += `- Not executed during pytest\n`;
            body += `- Invalid/non-existent file paths\n\n`;
          }
          
          body += `### 📊 Coverage Analysis Results\n\n`;
          body += '```\n' + coverageOutput + '\n```\n\n';
          
          if (!coveragePassed) {
            body += `### 🔧 How to Fix\n\n`;
            body += `**For "Not executed" tests:**\n`;
            body += `1. Make sure the test files are discoverable by pytest\n`;
            body += `2. Check that test functions are properly named (start with \`test_\`)\n`;
            body += `3. Ensure test files don't have syntax errors or import issues\n`;
            body += `4. Verify the test files are in the correct directory structure\n\n`;
            body += `**For "Invalid test files":**\n`;
            body += `1. Create the missing test files referenced in specifications\n`;
            body += `2. Update specification \`unit_test\` paths to point to existing files\n`;
            body += `3. Use correct relative paths from the repository root\n\n`;
            body += `**Testing locally:**\n`;
            body += `\`\`\`bash\n`;
            body += `# Run pytest and check coverage\n`;
            body += `python -m pytest tests/ -v > test_output.txt 2>&1\n`;
            body += `# Analyze with cdlreq (if available)\n`;
            body += `cdlreq coverage test_output.txt --directory .\n`;
            body += `\`\`\`\n\n`;
          }
          
          body += `---\n*This check ensures that all unit test files referenced in specifications are actually executed during testing.*`;
          
          // Find existing coverage comment and update it, or create new one
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const existingComment = comments.find(comment => 
            comment.user.type === 'Bot' && comment.body.includes('Test Coverage Validation')
          );
          
          if (existingComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: body
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });
          }

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-coverage-artifacts
        path: |
          test_output.txt
          coverage_analysis.py
        retention-days: 7

  # Summary job for branch protection  
  coverage-summary:
    runs-on: ubuntu-latest
    needs: [coverage-validation]
    if: always()
    
    steps:
    - name: Check coverage validation status
      run: |
        if [[ "${{ needs.coverage-validation.result }}" == "success" ]]; then
          echo "✅ Test coverage validation passed!"
          echo "All specification unit test files are properly covered."
          exit 0
        else
          echo "❌ Test coverage validation failed!"
          echo "Some specification unit test files are not executed or invalid."
          echo "This pull request cannot be merged until all issues are resolved."
          exit 1
        fi